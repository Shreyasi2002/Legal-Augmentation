{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Shreyasi2002/Legal-Augmentation/blob/main/LegalTextAugment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import locale\n",
        "def getpreferredencoding(do_setlocale = True):\n",
        "    return \"UTF-8\"\n",
        "locale.getpreferredencoding = getpreferredencoding"
      ],
      "metadata": {
        "id": "_hqXEWnVQHkt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch>=1.6.0 transformers>=4.11.3 sentencepiece"
      ],
      "metadata": {
        "id": "Yj5j2NRb0513",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0ce94c07-ad92-4931-ec4c-f88fe48895dd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[31mERROR: Operation cancelled by user\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nltk>=3.4.5"
      ],
      "metadata": {
        "id": "10QoSx__07pL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install numpy requests nlpaug "
      ],
      "metadata": {
        "id": "GUTIsJCv0_Fv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install sacremoses"
      ],
      "metadata": {
        "id": "ekULB8bg02MB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Loading the Data"
      ],
      "metadata": {
        "id": "8M_iCUsU0nPP"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5mbaWD5Z0MrW"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "with open('train.json') as f:\n",
        "    data = json.load(f)\n",
        "    f.close()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(data)"
      ],
      "metadata": {
        "id": "YRcZJMQJ0fT2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(data[0]['data']['text'])"
      ],
      "metadata": {
        "id": "SH4LN2J-y4pv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "res=data[0]['annotations'][0]['result']\n",
        "print(res)"
      ],
      "metadata": {
        "id": "m4Govml90g2C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Easy Data Augmentation (EDA"
      ],
      "metadata": {
        "id": "yhD5OAufUweL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Synonym Replacement"
      ],
      "metadata": {
        "id": "9seNQ0M8TUFX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load Data"
      ],
      "metadata": {
        "id": "usGdSTfbVpKy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "with open('train.json') as f:\n",
        "    data = json.load(f)\n",
        "    f.close()"
      ],
      "metadata": {
        "id": "CbJp8v5QVqNj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nlpaug.augmenter.word as naw\n",
        "aug = naw.SynonymAug(aug_src='wordnet')"
      ],
      "metadata": {
        "id": "CIxiuF5LVFnI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Synonym Replacement on single Document"
      ],
      "metadata": {
        "id": "ylKmvq6YSTei"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_new = data\n",
        "for i in range(len(res)):\n",
        "  text1 = res[i]['value']['text']\n",
        "  augmented_text = aug.augment(text1)\n",
        "  data_new[0]['annotations'][0]['result'][i]['value']['text'] = augmented_text\n",
        "  # print(augmented_text[0], '\\n')\n",
        "\n",
        "print(data_new[0])"
      ],
      "metadata": {
        "id": "X92yz7wMOBRM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Synonym Replacement on all docmuents"
      ],
      "metadata": {
        "id": "Gk--qSV5V4BZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import secrets\n",
        "import string\n",
        "\n",
        "# Function to generate random string of 32 character for ID\n",
        "def generate_id(N): \n",
        "  res = ''.join(secrets.choice(string.ascii_lowercase + string.digits) for i in range(N))\n",
        "  return res"
      ],
      "metadata": {
        "id": "0YQW5AA04rG8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for j in range(len(data)):\n",
        "  idx=0\n",
        "  data_str = \"\"\n",
        "  res = data[j]['annotations'][0]['result']\n",
        "  \n",
        "  for i in range(len(res)):\n",
        "    text1 = res[i]['value']['text']\n",
        "    augmented_text = aug.augment(text1)\n",
        "    data[j]['annotations'][0]['result'][i]['value']['text'] = augmented_text[0]\n",
        "\n",
        "    data[j]['annotations'][0]['result'][i]['value']['start'] = idx\n",
        "    data[j]['annotations'][0]['result'][i]['value']['end'] = idx + len(augmented_text[0])\n",
        "    idx = idx + len(augmented_text[0])\n",
        "    data[j]['annotations'][0]['result'][i]['id'] = generate_id(32)\n",
        "\n",
        "    if i>0:\n",
        "      data_str = str(data_str) + \" \" + str(augmented_text[0])\n",
        "    else:\n",
        "      data_str = str(data_str) + str(augmented_text[0])\n",
        "    \n",
        "  data[j]['data'] = data_str"
      ],
      "metadata": {
        "id": "4rsnQXfQhsLv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"sample-synonym_replacement.json\", \"w\") as outfile:\n",
        "    json.dump(data, outfile)"
      ],
      "metadata": {
        "id": "mpiA48qvibxt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Random Insertion"
      ],
      "metadata": {
        "id": "VB74L_FKWlaq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Random Deletion"
      ],
      "metadata": {
        "id": "eRImatd2Wo9P"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Contextual Word Embedding"
      ],
      "metadata": {
        "id": "dDvBmoiVSdKe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load Data"
      ],
      "metadata": {
        "id": "A2BQOwsOWeGY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "with open('train.json') as f:\n",
        "    data = json.load(f)\n",
        "    f.close()"
      ],
      "metadata": {
        "id": "RqmXJ01VVtCe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nlpaug.augmenter.word as naw\n",
        "aug = naw.ContextualWordEmbsAug(model_path='bert-base-uncased', action=\"substitute\")"
      ],
      "metadata": {
        "id": "16SSDmZqVMFJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Contextual Word Embedding on Single Document"
      ],
      "metadata": {
        "id": "ZYJsiNbVTQsy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_new = data\n",
        "for i in range(len(res)):\n",
        "  text1 = res[i]['value']['text']\n",
        "  augmented_text = aug.augment(text1)\n",
        "  data_new[0]['annotations'][0]['result'][i]['value']['text'] = augmented_text\n",
        "\n",
        "print(data_new[0])\n"
      ],
      "metadata": {
        "id": "BhPs5_LrQFBr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Contextual Embedding on All documents"
      ],
      "metadata": {
        "id": "eh_d1En5VzM-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import secrets\n",
        "import string\n",
        "\n",
        "# Function to generate random string of 32 character for ID\n",
        "def generate_id(N): \n",
        "  res = ''.join(secrets.choice(string.ascii_lowercase + string.digits) for i in range(N))\n",
        "  return res"
      ],
      "metadata": {
        "id": "QB6nGpAYV90L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for j in range(len(data)):\n",
        "  idx=0\n",
        "  data_str = \"\"\n",
        "  res = data[j]['annotations'][0]['result']\n",
        "  \n",
        "  for i in range(len(res)):\n",
        "    text1 = res[i]['value']['text']\n",
        "    augmented_text = aug.augment(text1)\n",
        "    data[j]['annotations'][0]['result'][i]['value']['text'] = augmented_text[0]\n",
        "\n",
        "    data[j]['annotations'][0]['result'][i]['value']['start'] = idx\n",
        "    data[j]['annotations'][0]['result'][i]['value']['end'] = idx + len(augmented_text[0])\n",
        "    idx = idx + len(augmented_text[0])\n",
        "    data[j]['annotations'][0]['result'][i]['id'] = generate_id(32)\n",
        "\n",
        "    if i>0:\n",
        "      data_str = str(data_str) + \" \" + str(augmented_text[0])\n",
        "    else:\n",
        "      data_str = str(data_str) + str(augmented_text[0])\n",
        "    \n",
        "  data[j]['data'] = data_str"
      ],
      "metadata": {
        "id": "dTbHScLBVAer"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"sample-contextual_embedding.json\", \"w\") as outfile:\n",
        "    json.dump(data, outfile)"
      ],
      "metadata": {
        "id": "hNmNJhB4VboK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Backtranslation"
      ],
      "metadata": {
        "id": "FmXyAwdF0x8L"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Testing BT on single sentence"
      ],
      "metadata": {
        "id": "r-veZJjAWMwf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nlpaug.augmenter.word as naw\n",
        "\n",
        "text2='this appeal coming on for hearing this day, the court delivered the following : - judgment heard the learned counsel for the appellant and the learned government pleader. 2. the accused is in appeal in the following circumstances : - the appellant was accused of offences punishable under sections 498a and 306 of the indian penal code, 1860 ( hereinafter referred to as the ` ipc \\', for brevity )'\n",
        "back_translation_aug = naw.BackTranslationAug( from_model_name='facebook/wmt19-en-de', to_model_name='facebook/wmt19-de-en')\n",
        "augmented_text1 = back_translation_aug.augment(text2)"
      ],
      "metadata": {
        "id": "lPSd8utc0ka7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Augmented Text:\")\n",
        "print(augmented_text1)"
      ],
      "metadata": {
        "id": "dqlnQN5SGpnC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load Data"
      ],
      "metadata": {
        "id": "EkFpBkeMWZ-Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "with open('train.json') as f:\n",
        "    data = json.load(f)\n",
        "    f.close()\n",
        "\n",
        "res=data[0]['annotations'][0]['result']"
      ],
      "metadata": {
        "id": "JWsCzuH1WZVW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nlpaug.augmenter.word as naw\n",
        "back_translation_aug = naw.BackTranslationAug( from_model_name='facebook/wmt19-en-de', to_model_name='facebook/wmt19-de-en')"
      ],
      "metadata": {
        "id": "_O7NNRlJXPPR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "BT on Single Document"
      ],
      "metadata": {
        "id": "9-OuYcwaXTnw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_new = data\n",
        "for i in range(len(res)):\n",
        "  text1 = res[i]['value']['text']\n",
        "  augmented_text = back_translation_aug.augment(text1)\n",
        "  data_new[0]['annotations'][0]['result'][i]['value']['text'] = augmented_text\n",
        "\n",
        "print(data_new[0])"
      ],
      "metadata": {
        "id": "3fhErCGBXMZt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "BT on all documents"
      ],
      "metadata": {
        "id": "yPvdfK1GXchc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import secrets\n",
        "import string\n",
        "\n",
        "# Function to generate random string of 32 character for ID\n",
        "def generate_id(N): \n",
        "  res = ''.join(secrets.choice(string.ascii_lowercase + string.digits) for i in range(N))\n",
        "  return res"
      ],
      "metadata": {
        "id": "l82NAhrxXZTB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for j in range(len(data)):\n",
        "  idx=0\n",
        "  data_str = \"\"\n",
        "  res = data[j]['annotations'][0]['result']\n",
        "  \n",
        "  for i in range(len(res)):\n",
        "    text1 = res[i]['value']['text']\n",
        "    augmented_text = back_translation_aug.augment(text1)\n",
        "    data[j]['annotations'][0]['result'][i]['value']['text'] = augmented_text[0]\n",
        "\n",
        "    data[j]['annotations'][0]['result'][i]['value']['start'] = idx\n",
        "    data[j]['annotations'][0]['result'][i]['value']['end'] = idx + len(augmented_text[0])\n",
        "    idx = idx + len(augmented_text[0])\n",
        "    data[j]['annotations'][0]['result'][i]['id'] = generate_id(32)\n",
        "\n",
        "    if i>0:\n",
        "      data_str = str(data_str) + \" \" + str(augmented_text[0])\n",
        "    else:\n",
        "      data_str = str(data_str) + str(augmented_text[0])\n",
        " \n",
        "  data[j]['data'] = data_str"
      ],
      "metadata": {
        "id": "5I_07D47XhWO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"sample-backtranslation.json\", \"w\") as outfile:\n",
        "    json.dump(data, outfile)"
      ],
      "metadata": {
        "id": "aRizzLX3XtRN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### SMOTE"
      ],
      "metadata": {
        "id": "AykivlzhIAcZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd"
      ],
      "metadata": {
        "id": "owzJChnRKnJb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# train_data = []\n",
        "train_data_txt = []\n",
        "train_data_lbl = []\n",
        "\n",
        "for j in range(len(data)):\n",
        "  res = data[j]['annotations'][0]['result']\n",
        "  for i in range(len(res)):\n",
        "    train_data_txt.append(res[i]['value']['text'])\n",
        "    train_data_lbl.append(res[i]['value']['labels'][0])\n",
        "  \n",
        "train_df = pd.DataFrame({'text': train_data_txt, 'label': train_data_lbl})\n",
        "\n",
        "print(train_df)"
      ],
      "metadata": {
        "id": "aaJ6nH38JWVo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from imblearn.over_sampling import SMOTE\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# Convert the text data to numerical features\n",
        "tfidf = TfidfVectorizer()\n",
        "X = tfidf.fit_transform(train_df['text'])\n",
        "\n",
        "# Apply SMOTE to the numerical features\n",
        "smote = SMOTE()\n",
        "X_resampled, y_resampled = smote.fit_resample(X, train_df['label'])\n",
        "\n",
        "# Convert the synthetic samples to text\n",
        "resampled_text = [tfidf.inverse_transform(sample) for sample in X_resampled]\n",
        "\n",
        "# Combine the original and synthetic samples\n",
        "resampled_data = pd.DataFrame({'text': resampled_text, 'label': y_resampled})\n",
        "\n",
        "print(resampled_data)"
      ],
      "metadata": {
        "id": "Be4Fl2P1H-p8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## GPT2"
      ],
      "metadata": {
        "id": "P8EdQxQax6fU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers\n"
      ],
      "metadata": {
        "id": "KOxRWzvsx6GZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
        "import torch\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "model = GPT2LMHeadModel.from_pretrained('gpt2-medium').to(device)\n",
        "tokenizer = GPT2Tokenizer.from_pretrained('gpt2-medium')"
      ],
      "metadata": {
        "id": "7KrIt4q7EZl7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "with open('train.json') as f:\n",
        "    data = json.load(f)\n",
        "    f.close()"
      ],
      "metadata": {
        "id": "fYPElkhkE5W5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "res=data[0]['annotations'][0]['result']\n",
        "# input_text = res[0]['value']['text']\n",
        "input_text = 'It is claimed that the accused had looked after Lakshmi well till the birth of their first child, which was a girl and he was not happy with the girl child and thereafter started ill-treating Lakshmi, apart from which, he was also demanding dowry and on that pretext, he used to assault her continuously.\\n'\n",
        "print('Original Text: \\n'+input_text)\n",
        "input_ids = tokenizer.encode(input_text, return_tensors='pt').to(device)\n",
        "output_ids = model.generate(\n",
        "  input_ids.to(device),\n",
        "  max_length=100,\n",
        "  do_sample=True,\n",
        "  num_beams=5, \n",
        "  no_repeat_ngram_size=3, \n",
        "  early_stopping=True\n",
        ")\n",
        "output_ids = output_ids.to('cpu')\n",
        "output_text = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
        "print('Synthetic Text: \\n'+output_text)"
      ],
      "metadata": {
        "id": "kdB6tCxPJU8p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "GPT2 Paraphrasing on Single Document"
      ],
      "metadata": {
        "id": "IjKSHkqnFIyq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "res=data[0]['annotations'][0]['result']\n",
        "data_new = data\n",
        "for i in range(len(res)):\n",
        "  input_text = res[i]['value']['text']\n",
        "  input_ids = tokenizer.encode(input_text, return_tensors='pt').to(device)\n",
        "  output_ids = model.generate(\n",
        "    input_ids.to(device),\n",
        "    max_length=100,\n",
        "    do_sample=True,\n",
        "    top_k=50,\n",
        "    temperature=0.7\n",
        "  )\n",
        "  output_ids = output_ids.to('cpu')\n",
        "  output_text = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
        "  data_new[0]['annotations'][0]['result'][i]['value']['text'] = output_text\n",
        "\n",
        "print(data_new[0])\n"
      ],
      "metadata": {
        "id": "2qO7og39FIyq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "GPT2 Paraphrasing on All documents"
      ],
      "metadata": {
        "id": "VBMFHMpmFIyq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import secrets\n",
        "import string\n",
        "\n",
        "# Function to generate random string of 32 character for ID\n",
        "def generate_id(N): \n",
        "  res = ''.join(secrets.choice(string.ascii_lowercase + string.digits) for i in range(N))\n",
        "  return res"
      ],
      "metadata": {
        "id": "WwJQWJOZFIyq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for j in range(len(data)):\n",
        "  idx=0\n",
        "  data_str = \"\"\n",
        "  res = data[j]['annotations'][0]['result']\n",
        "  \n",
        "  for i in range(len(res)):\n",
        "    text1 = res[i]['value']['text']\n",
        "    augmented_text = aug.augment(text1)\n",
        "    data[j]['annotations'][0]['result'][i]['value']['text'] = augmented_text[0]\n",
        "\n",
        "    data[j]['annotations'][0]['result'][i]['value']['start'] = idx\n",
        "    data[j]['annotations'][0]['result'][i]['value']['end'] = idx + len(augmented_text[0])\n",
        "    idx = idx + len(augmented_text[0])\n",
        "    data[j]['annotations'][0]['result'][i]['id'] = generate_id(32)\n",
        "\n",
        "    if i>0:\n",
        "      data_str = str(data_str) + \" \" + str(augmented_text[0])\n",
        "    else:\n",
        "      data_str = str(data_str) + str(augmented_text[0])\n",
        "    \n",
        "  data[j]['data'] = data_str"
      ],
      "metadata": {
        "id": "Zwm0i-DrFIyr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"sample-paraphrase.json\", \"w\") as outfile:\n",
        "    json.dump(data, outfile)"
      ],
      "metadata": {
        "id": "EpRugX1rFIyr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Using T5 model"
      ],
      "metadata": {
        "id": "beVIPGThOePA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install sentencepiece\n",
        "!pip install torch\n",
        "!pip install transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aQrStowTP9jI",
        "outputId": "98406db2-2f2d-4aa4-bcf7-6b34bf4a8708"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting sentencepiece\n",
            "  Downloading sentencepiece-0.1.97-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m22.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: sentencepiece\n",
            "Successfully installed sentencepiece-0.1.97\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.9/dist-packages (1.13.1+cu116)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.9/dist-packages (from torch) (4.5.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.27.4-py3-none-any.whl (6.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.8/6.8 MB\u001b[0m \u001b[31m26.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from transformers) (3.10.7)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from transformers) (2.27.1)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
            "  Downloading tokenizers-0.13.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.6/7.6 MB\u001b[0m \u001b[31m48.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.9/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.9/dist-packages (from transformers) (1.22.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from transformers) (23.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.9/dist-packages (from transformers) (4.65.0)\n",
            "Collecting huggingface-hub<1.0,>=0.11.0\n",
            "  Downloading huggingface_hub-0.13.3-py3-none-any.whl (199 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m199.8/199.8 KB\u001b[0m \u001b[31m19.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.9/dist-packages (from transformers) (2022.10.31)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (2.0.12)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (2022.12.7)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (3.4)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (1.26.15)\n",
            "Installing collected packages: tokenizers, huggingface-hub, transformers\n",
            "Successfully installed huggingface-hub-0.13.3 tokenizers-0.13.2 transformers-4.27.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "with open('train.json') as f:\n",
        "    data = json.load(f)\n",
        "    f.close()"
      ],
      "metadata": {
        "id": "TLetfSv3UASB",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "outputId": "05539a9c-8127-428b-c5d1-78bf82ff72ae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-54c640b95dc4>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'train.json'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'train.json'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Summarize"
      ],
      "metadata": {
        "id": "1UiZV1vPplpT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import T5ForConditionalGeneration, T5Tokenizer\n",
        "import torch\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "model_name = 't5-base'\n",
        "tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
        "model = T5ForConditionalGeneration.from_pretrained(model_name).to(device)\n",
        "\n",
        "# res=data[0]['annotations'][0]['result']\n",
        "# input_text = res[0]['value']['text']\n",
        "input_text = 'It is claimed that the accused had looked after Lakshmi well till the birth of their first child, which was a girl and he was not happy with the girl child and thereafter started ill-treating Lakshmi, apart from which, he was also demanding dowry and on that pretext, he used to assault her continuously.\\n'\n",
        "print(\"Original Text: \\n\"+input_text)\n",
        "task_prefix = 'Summarize: \\n'\n",
        "inputs = tokenizer(task_prefix+input_text, return_tensors='pt').to(device)\n",
        "output_ids = model.generate(\n",
        "  input_ids=inputs[\"input_ids\"].to(device),\n",
        "  do_sample=True,\n",
        "  max_length=100,\n",
        "  num_beams=5, \n",
        "  no_repeat_ngram_size=3, \n",
        "  early_stopping=True\n",
        ")\n",
        "output_ids = output_ids.to('cpu')\n",
        "output_text = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
        "print(\"Output Text: \\n\"+output_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wHe5SL-mOdzx",
        "outputId": "180fb3c4-ad5c-4dfa-d403-4e15cb428586"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Text: \n",
            "It is claimed that the accused had looked after Lakshmi well till the birth of their first child, which was a girl and he was not happy with the girl child and thereafter started ill-treating Lakshmi, apart from which, he was also demanding dowry and on that pretext, he used to assault her continuously.\n",
            "\n",
            "Output Text: \n",
            "Lakshmi's father allegedly started ill-treating her and demanding dowry. on that pretext, he used to assault her continuously.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Back Translation"
      ],
      "metadata": {
        "id": "rt0OXVuNpntq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import T5ForConditionalGeneration, T5Tokenizer\n",
        "import torch\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "model_name = 't5-base'\n",
        "tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
        "model = T5ForConditionalGeneration.from_pretrained(model_name).to(device)\n",
        "\n",
        "# res=data[0]['annotations'][0]['result']\n",
        "# input_text = res[0]['value']['text']\n",
        "input_text = 'It is claimed that the accused had looked after Lakshmi well till the birth of their first child, which was a girl and he was not happy with the girl child and thereafter started ill-treating Lakshmi, apart from which, he was also demanding dowry and on that pretext, he used to assault her continuously.\\n'\n",
        "print(\"Original Text: \\n\"+input_text)\n",
        "\n",
        "# Translating to German\n",
        "task_prefix1 = 'translate English to French: '\n",
        "inputs = tokenizer(task_prefix1+input_text, return_tensors='pt').to(device)\n",
        "output_ids1 = model.generate(\n",
        "  input_ids=inputs[\"input_ids\"].to(device),\n",
        "  do_sample=True,\n",
        "  max_length=100,\n",
        "  num_beams=4, \n",
        "  no_repeat_ngram_size=3, \n",
        "  early_stopping=True\n",
        ")\n",
        "output_ids1 = output_ids1.to('cpu')\n",
        "output_text1 = tokenizer.decode(output_ids1[0], skip_special_tokens=True)\n",
        "print(\"First Translation: \\n\"+output_text1)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6fcaeadc-8940-48c1-e680-9ac042adba90",
        "id": "aX9ZfqLspkOL"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Text: \n",
            "It is claimed that the accused had looked after Lakshmi well till the birth of their first child, which was a girl and he was not happy with the girl child and thereafter started ill-treating Lakshmi, apart from which, he was also demanding dowry and on that pretext, he used to assault her continuously.\n",
            "\n",
            "First Translation: \n",
            "Il est allégué que l'accusé s'était bien occupé de Lakshmi jusqu'à la naissance de leur premier enfant, qui était une fille, et qu'il n'était pas satisfait de cette fille; par la suite, il a commencé à la maltraiter, en dehors de quoi il exigeait également la dot en lui\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Translating Back to English\n",
        "task_prefix2 = 'translate to English: '\n",
        "input_text2 = output_text1\n",
        "inputs2 = tokenizer(task_prefix2+input_text2, return_tensors='pt').to(device)\n",
        "output_ids2 = model.generate(\n",
        "  input_ids=inputs2[\"input_ids\"].to(device),\n",
        "  do_sample=True,\n",
        "  max_length=100,\n",
        "  num_beams=4, \n",
        "  no_repeat_ngram_size=3, \n",
        "  early_stopping=True\n",
        ")\n",
        "output_ids2 = output_ids2.to('cpu')\n",
        "output_text2 = tokenizer.decode(output_ids2[0], skip_special_tokens=True)\n",
        "print(\"Output Text: \\n\"+output_text2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OOPYPC8b4RnK",
        "outputId": "2f4a0e2d-117b-42a8-9d75-619983d1693b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output Text: \n",
            "Es ist allégué, dass Lakshmi bis zur Geburt ihres ersten Kindes, das eine Tochter war, gut erzogen wurde und dass er nicht zufrieden war mit dieser Tochter; als Folge hat er angefangen, mal zu handeln, außerhalb dessen, was er auch den Punkt in ihm verlangte.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Random Insertions using BERT"
      ],
      "metadata": {
        "id": "-AcjRrUUXDnA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
        "\n",
        "import torch\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"law-ai/InLegalBERT\")\n",
        "\n",
        "model = AutoModelForMaskedLM.from_pretrained(\"law-ai/InLegalBERT\").to(device)"
      ],
      "metadata": {
        "id": "MapAYI3LOKFG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "with open('train.json') as f:\n",
        "    data = json.load(f)\n",
        "    f.close()"
      ],
      "metadata": {
        "id": "ax6X1asuXJ-_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "def mask_words(text, mask_prob=0.15):\n",
        "  words = text.split()\n",
        "  output_words = []\n",
        "  for word in words:\n",
        "      if random.uniform(0, 1) < mask_prob:\n",
        "          output_words.append('[MASK]')\n",
        "      output_words.append(word)\n",
        "  return ' '.join(output_words)"
      ],
      "metadata": {
        "id": "4oHNctj_bHpU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to paraphrase a sentence using random masks and BERT\n",
        "def paraphrase(text):\n",
        "    masked_text = mask_words(text, 0.2)\n",
        "    print(\"Masked Text: \\n\"+masked_text)\n",
        "    tokens = tokenizer.tokenize(masked_text)\n",
        "    masked_indices = [i for i, x in enumerate(tokens) if x == '[MASK]']\n",
        "    if len(masked_indices) == 0:\n",
        "      return text\n",
        "    input_ids = tokenizer.encode(masked_text, return_tensors='pt').to(device)\n",
        "    with torch.no_grad():\n",
        "      outputs = model(input_ids)  \n",
        "    predicted_tokens = []\n",
        "    for i in masked_indices:\n",
        "      predicted_index = torch.argmax(outputs[0][0][i]).item()\n",
        "      predicted_token = tokenizer.convert_ids_to_tokens([predicted_index])[0]\n",
        "      predicted_tokens.append(predicted_token)\n",
        "    output_tokens = tokens.copy()\n",
        "    for i, index in enumerate(masked_indices):\n",
        "      output_tokens[index] = predicted_tokens[i]\n",
        "    output_text = tokenizer.convert_tokens_to_string(output_tokens)\n",
        "    return output_text"
      ],
      "metadata": {
        "id": "GKnUc4J5XJ_A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "old_text = 'In this regard, it is the complainant\\'s case that Laxmi had repeatedly told her about the ill-treatment.'\n",
        "print(\"Original Text: \\n\"+old_text)\n",
        "new_text = paraphrase(old_text)\n",
        "print(\"Synthetic Text: \\n\"+new_text)"
      ],
      "metadata": {
        "id": "MFz1A4iFYNMv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wBEG3_9ilqNu"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}